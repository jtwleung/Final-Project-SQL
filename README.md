# Final-Project-Transforming-and-Analyzing-Data-with-SQL

## Project/Goals
This is the SQL Module Final Project for Lighthouse Labs' Immersive Data Science Bootcamp.

The goals of this project are to put together all the component knowledge about SQL that has been learned in the first 14 days of the bootcamp, including the following tasks:
- extracting data from a SQL database
- cleaning, transforming and analyzing data
- loading data into a database
- developing and implementing a QA process to validate transformed data against raw data

## Process
### 1. Examine Data within Excel in CSV format
### 2. Determine recipient tables for the Excel CSV files
### 3. Determine datatypes for columns in the recipient tables
### 4. Import 4 CSV files into 4 tables in PostgreSQL
### 5. Examine data within columns of each table, and run preliminary EDA SQL queries, to determine data cleaning steps for each table
### 6. Create 3 new views for 3 out of 4 tables with data cleaning steps implemented.  Data cleaning steps included such things as:
#### a. Removing columns with no data in any rows
#### b. String manipulations to remove erroneous data
1. Removing extranneous and erroneous words like "Google", "YouTube", "Android", "Waze" from product names
2. Removing an erroneous single entry in the pagetitle column
3. De-duplicating data (e.g. setting "/asearch.html/" to "/asearch.html", etc.)
4. Removing leading and traliing whitespaces from product name
#### c. Standardizing / normalizing / scaling very large values for price/revenue (i.e. dividing by 1,000,000)
#### d. Interpolating data (replacing NULL values for sentimentscore and sentimentmagnitude with average values - please note that this was done "on the fly" during SQL querying, on a subset of rows)
#### e. Making note of rows of data that require further investigation with members of the business to fill in missing data
### 7. Writing and executing SQL to carry-out QA to ensure cleaned table integrity and consistency with original data, after cleaning
### 8. Writing and executing SQL to carry-out EDA to understand how the tables and keys are related in the data, given that there is no data dictionary available for the data
### 9. Writing and executing SQL to answer questions posed in the "starting_with_questions"
### 10. Formulating additional questions about the data, recorded within "starting_with_data", and writing and executing SQL to answer those questions

## Results
This data can tell us:
1. All visits to the ecommerce site, including those which resulted in sales
2. Marketing channel that a session came through.
3. Session statistics such as date of visit, time on site, ecommerce actions taken, city and country the session was initiated from, headings/groupings of the web pages on the ecommerce site visited, etc.
4. Website analytics information such as number of page views, unique visit id, type of social engagement, etc.
5. Product SKU, name, unit price, quantity ordered, quantity in stock, sentiment towards product, product category.
6. Sales information, specifically units sold.

Combining these pieces of data together can give us an indication of amount of revenue/sales generated by certain cities and countries, level of engagement of our customers, behaviour while on website of our customers, product stock, product orders, date of orders, dates of visits to the website, numbers of visits that do not result in sales.

Analytics on this data can give us insights on things such as:
1. Countries (or city and country combinations) generating the highest revenue, top products ordered per city/country, top selling product in each city/country, average number of products ordered per city/country.
2. What products we are needing to fulfill that are out of stock, and how urgently we need to procure more stock.
3. Most effective marketing channels to generate revenue and what channel we might invest next to more effectively increase our revenue.
4. Top 10 best selling products across all countries orders were placed in.
5. Trends/patterns in site visits over certain periods of the year, and where data is lacking for that analysis.

### Specifically, here are my conclusions:
#### From the questions I as the Consultant asked (starting_with_data.md):
1. We have an inventory problem.  Inventory and Procurement processes could benefit from process improvement and/or re-engineering to mitigate customer satisfaction impact, given the sheer number (15) of products with significant negative surplus values.
2. Purchasing Department top priority should be procuring more of GGOEGFSR022099: “Kick Ball” from both: 1) absolute number owing, 2) number owing relative to number ordered.
3. The 2 most effective marketing channels are “Referral” and “Direct”.  Channels are top by both total number of units sold and average price of units sold.  Continue with channel “development & feeding” approaches.
4. “Affiliates” channel has lowest ranking on total units sold, but 3rd highest average price.  Possible quick win is to invest in “Affiliates” channel development.  Small adjustments targeted towards increasing volume sold could have disproportionate impact on overall remit of this channel.
5. “Top 10 Products Sold” based on total units differs significantly when generating this data from different data sources (1) analytics (unitssold), 2) salesbysku (total_ordered) or 3) products (orderedquantity).
- Indoor Security Camera in top 10 regardless of data source.  Indicates validity?
- Wildly different result sets highlight degree of data inconsistency due to differing date ranges, and problems with referential integrity and data integrity.
- Invites a revisit with cleaner data and additional investigation time.

#### From the questions the Client asked (starting_with_questions.md):
1.  Highest transaction revenue (driven by “unitssold” values from analytics) in “Unknown” location  shows extent of missing location data.
2.  Top 9 highest transactions from US  possibilities for growth from other locations. potential relative saturation in US.
3.  No obvious patterns apparent to “naked eye” for the below questions (result sets in report)  follow-on statistical or graphical analysis valuable; definite need for data integrity improvements (analytics.unitssold averages 1.xx causes switch to using quantities from allsessions, products, salesbysku):
- Average number products ordered each city, country
- Categories of products ordered from each city, country
- Best-selling products per city, country
- Effect of Revenue on “Impact” (using sentimentscores * sentimentmagnitudes * quantity)
4.  “Revenue effect on Impact?”  value in obtaining true impact data such as dollars invested in community infrastructure, dollars invested in charity, employment statistics in countries in which we operate.


## Challenges 
This project had several challenges:

1. No data dictionary to explain what the columns mean, and how the columns/keys and tables relate to each other.  This caused a lot of guessing and caveats on the validity of the conclusions from the data analysis.
2. No "business" to ask questions about the data, the application, domain, context under which this data was gathered and how the data might be used to draw conclusions.  No ability to validate that the application of the analysis would be correct.
3. Short time-frame under which to complete the project (done over 48 hours non-stop).
4. Data quality is extremely poor; many null values, a lot of inconsistencies in data, corrupt data, numbers and sums do not add up or agree between tables.
5. Not possible to add Primary Key-Foreign Key constraints due to data quality.  Please note that the ERD ("schema.png") has been drawn in such a way as to show potential relationships between tables even though they do not adhere to PK-FK relationships guaranteeing referential integrity. Please see the note on the "schema.png" diagram.

## Future Goals

If I had more time and tools, I would:

1. Do further data cleaning after more EDA type interrogation of the data.  Include, for example, Outlier detection and removal in various parts of the entire dataset.
2. Find a "business person" to interview to ask more questions about the way this data is sourced, the business domain, and to validate that the assumptions I have made about how the data connects, is accurate.
3. Take some of my findings from the LHL questions and do statistical analysis looking for patterns and trends that are more subtle than stand out to the "naked eye".  This would also include doing graphical analysis (visualizations, histograms, etc.) of the results.
4. Follow up with "the business" about the tentative conclusions I drew from the questions I posed to the data, in order to get feedback and see if further data analysis or further data procurement would help with developing further and stronger conclusions.
